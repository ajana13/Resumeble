{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\quynh\\anaconda3\\lib\\site-packages (20200517)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.1.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.9.8)\n",
      "Requirement already satisfied: docx2txt in c:\\users\\quynh\\anaconda3\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: spacy in c:\\users\\quynh\\anaconda3\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\quynh\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\quynh\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six\n",
    "!pip install docx2txt\n",
    "!pip install spacy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_sm\n",
    "#python -m nltk nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import io\n",
    "import nltk\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_doc(doc_path):\n",
    "    '''\n",
    "    Helper function to extract plain text from .doc or .docx files\n",
    "    :param doc_path: path to .doc or .docx file to be extracted\n",
    "    :return: string of extracted text\n",
    "    '''\n",
    "    temp = docx2txt.process(doc_path)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule based matching to find\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initalize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [[{'POS': 'PROPN'}, {'POS': 'PROPN'}]]\n",
    "    \n",
    "    matcher.add('NAME', None, *pattern)\n",
    "    \n",
    "    print(matcher)\n",
    "    print(pattern)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "#As you can observe above, we have first defined a pattern that we want \n",
    "# to search in our text. Here, we have created a simple pattern based\n",
    "# on the fact that First Name and Last Name of a person is always a\n",
    "# Proper Noun. Hence we have specified spacy that searches for a \n",
    "# pattern such that two continuous words whose part of speech tag \n",
    "# is equal to PROPN (Proper Noun)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Step: Extracting Phone Numbers\n",
    "\n",
    "For extracting phone numbers, we will be making use of regular expressions. Phone numbers also have multiple forms such as (+91) 1234567890 or +911234567890 or +91 123 456 7890 or +91 1234567890. Hence, we need to define a generic regular expression that can match all similar combinations of phone numbers. Thanks to this blog, I was able to extract phone numbers from resume text by making slight tweaks.\n",
    "\n",
    "Our phone number extraction function will be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Step: Extracting Email\n",
    "\n",
    "For extracting Email IDs from resume, we can use a similar approach that we used for extracting mobile numbers. Email IDs have a fixed form i.e. an alphanumeric string should follow a @ symbol, again followed by a string, followed by a . (dot) and a string at the end. We can use regular expression to extract such expression from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth Step: Extracting Skills\n",
    "Now that we have extracted some basic information about the person, lets extract the thing that matters the most from a recruiter point of view, i.e. skills. We can extract skills using a technique called tokenization. Tokenization simply is breaking down of text into paragraphs, paragraphs into sentences, sentences into words. Hence, there are two major techniques of tokenization: Sentence Tokenization and Word Tokenization.\n",
    "\n",
    "Before implementing tokenization, we will have to create a dataset against which we can compare the skills in a particular resume. For this we will make a comma separated values file (.csv) with desired skillsets. For example, if I am the recruiter and I am looking for a candidate with skills including NLP, ML, AI then I can make a csv file with contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we gave the above file, a name as skills.csv, we can move further to tokenize our extracted text and compare the skills against the ones in skills.csv file. For reading csv file, we will be using the pandas module. After reading the file, we will removing all the stop words from our resume text. In short, a stop word is a word which does not change the meaning of the sentence even if it is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def extract_skills(resume_text):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    nlp_text = nlp(resume_text)\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # Reading the csv file\n",
    "    data = pd.read_csv(\"skills.csv\")\n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "        \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Javascript', 'Design', 'Pandas', 'Software engineering', 'Mobile', 'Analytics', 'R', 'Tensorflow', 'Prototype', 'Python', 'Js', 'Css', 'Ui', 'Algorithms', 'System', 'Docker', 'Analyze', 'Transportation', 'Html', 'Github', 'Json', 'Programming', 'Linux', 'C', 'Statistics', 'Jira', 'Java', 'Architecture', 'Numpy', 'Automation', 'Cloud', 'Research', 'Vmware', 'Engineering', 'Computer science', 'Rest', 'Spark', 'Sql']\n"
     ]
    }
   ],
   "source": [
    "print(extract_skills(resume_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth Step: Extracting Education:\n",
    "Now, moving towards the last step of our resume parser, we will be extracting the candidates education details. The details that we will be specifically extracting are the degree and the year of passing. For example, XYZ has completed MS in 2018, then we will be extracting a tuple like ('MS', '2018'). For this we will be requiring to discard all the stop words. We will be using nltk module to load an entire list of stopwords and later on discard those from our resume text.\n",
    "\n",
    "Recruiters are very specific about the minimum education/degree required for a particular job. Hence, we will be preparing a list EDUCATION that will specify all the equivalent degrees that are as per requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII', 'Bachelors of Science'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path, extension):\n",
    "    '''\n",
    "    Wrapper function to detect the file extension and call text extraction function accordingly\n",
    "    :param file_path: path of file of which text is to be extracted\n",
    "    :param extension: extension of file `file_name`\n",
    "    '''\n",
    "    text = ''\n",
    "    if extension == '.pdf':\n",
    "        for page in extract_text_from_pdf(file_path):\n",
    "            text += ' ' + page\n",
    "    elif extension == '.docx' or extension == '.doc':\n",
    "        text = extract_text_from_doc(file_path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = \"TimothyNguyen2022.pdf\"\n",
    "resume_text = extract_text(resume, os.path.splitext(resume)[1])\n",
    "#ans = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "6178883076\n",
      "<spacy.matcher.matcher.Matcher object at 0x0000026E1F541348>\n",
      "[[{'POS': 'PROPN'}, {'POS': 'PROPN'}]]\n",
      "Timothy Nguyen\n"
     ]
    }
   ],
   "source": [
    "print(extract_education(resume_text))\n",
    "print(extract_mobile_number(resume_text))\n",
    "print(extract_name(resume_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Timothy Nguyen\n",
      "quynhthoa1972@gmail.com · 617-888-3076 · www.linkedin.com/in/timothy-nguyen-414525144 · https://timothynguyen.github.io/TimothyNguyen\n",
      "EDUCATION\n",
      "University of Massachusetts, Amherst\n",
      "Bachelor of Science in Computer Science, Statistics & Data Science\n",
      "GPA: 3.835\n",
      "Relevant Coursework: Algorithms, Data Structures, Software Engineering, Scalable Web Systems, Computer Network Theory,\n",
      "Databases, Information Retrieval, Data Science, Natural Language Processing, Artificial Intelligence\n",
      "\n",
      "Amherst, MA\n",
      "Graduation: May 2022\n",
      "\n",
      "WORK EXPERIENCE\n",
      "MathWorks\n",
      "Incoming Software Engineering Intern\n",
      "\n",
      "Sept 2021 - Dec 2021\n",
      "\n",
      "May 2021 - Aug 2021\n",
      "\n",
      "● Modernizing & Developing JavaScript Web Components for MathWorks products with HTML, CSS, JS & React.\n",
      "\n",
      "Dell Technologies\n",
      "Software Engineering Intern for the Edge Solutions Business Unit\n",
      "\n",
      "● Collaborated to prototype edge-computing platforms with Litmus Edge and VMware to optimize the manufacturing industry.\n",
      "● Capture, analyze, visualize, and manage industrial data with Python, Tensorflow, JS, Docker, Postgres, TimescaleDB, and Grafana.\n",
      "● Innovated ML/Time Series models such as Kalman Filters and SARIMAX to estimate noisy trends from industrial MQTT sensors.\n",
      "● Monitored and performance tested Litmus Edge within VMware, supporting up to 10,000 industrial automation devices.\n",
      "\n",
      "Systems & Technology Research\n",
      "Software Engineering Intern for the Data Analytics Division\n",
      "\n",
      "May 2020 - Aug 2020\n",
      "\n",
      "● Designed a Spring Boot/Java REST service to rapidly automate assurance cases & causal models for predictive maintenance.\n",
      "● Developed Bayesian Networks to support causal model architecture and Uber Model Manager to manage different models.\n",
      "\n",
      "BUILD UMass\n",
      "Software Engineer\n",
      "\n",
      "Sept 2019 - Present\n",
      "Github: https://git.io/JLpVa\n",
      "\n",
      "● Provide non-profits, startups, and local businesses with web and mobile applications through pro-bono engagements.\n",
      "● Grew organization to over 50 software developers, 20 business developers, and partnered with over 10 clients over two years.\n",
      "● Led two teams of 8 people with the cofounder of BUILD UMass to develop an auth/user management & forms/surveys system\n",
      "● Developing JSON form-building application to customize form-making, filling, as well as drag-and-drop capabilities.\n",
      "● Employed React for front-end UI, Node/Express.js/MongoDB for backend, along with Redux & Passport/JWT Integration.\n",
      "\n",
      "DSC-WAV (National Science Foundation)\n",
      "Data Scientist Intern\n",
      "\n",
      "Sept 2020 - Jan 2021\n",
      "Github: https://git.io/JLpah\n",
      "\n",
      "● Research to provide better transportation for people with drug addiction to get to treatment centers on an eight-person team.\n",
      "● Developed visualizations and analyzed the cost-time benefit to get to treatment centers with Python and Google Cloud.\n",
      "\n",
      "PERSONAL PROJECTS\n",
      "Movie Recommender System\n",
      "\n",
      "Github: https://git.io/J3J9K\n",
      "● Established features such as authentication, movie pages, and movie recommendations through ALS & Collaborative Filtering.\n",
      "● Developed microservices through React, TypeScript, Node/Express, Postgres, Docker, Nginx, FastAPI, and Spark.\n",
      "\n",
      "Project Evaluation Tool\n",
      "\n",
      "Github: https://git.io/JLpVe\n",
      "\n",
      "● Used React, Node.js, Express, MongoDB, and Redux to create simple forms and manage users  for Ultimate Software.\n",
      "● Collaborate on an eight person team to develop mockups, user stories, entity relationship diagrams, and design docs.\n",
      "\n",
      "Covid.io\n",
      "\n",
      "Github: https://git.io/JLpab\n",
      "\n",
      "● Developed a Covid-19 dashboard related to cases and deaths with React.js and Charts.js.\n",
      "● Evaluate political classification through ML/Deep Learning Models on 200,000 covid-19 related tweets from US politicians.\n",
      "\n",
      "SKILLS\n",
      "● Programming Languages: Java, Python, JavaScript, R, C, HTML/CSS, SQL, Linux\n",
      "● Frameworks:  React, Node/Express, Redux, Spring Boot, React Native, NumPy, Pandas, Tensorflow\n",
      "● Devops: Docker, Nginx, Git, Jira, Postgres, InfluxDB, TimeScaleDB, Grafana.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
